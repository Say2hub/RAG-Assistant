# -*- coding: utf-8 -*-
"""Agentic AI Project-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzFpc6ewHyA55MPv_Z1m-XBgIV1nWehj
"""

#Install dependencies
!pip install langchain transformers torch faiss-cpu huggingface_hub openai gradio

!pip install sentence_transformers

from google.colab import files

uploaded = files.upload()
file = next(iter(uploaded.keys()))
print(f"Files:{file}")

import json
with open(file,'r') as f:
  data=json.load(f)
print("First 2 entries:")
for i, entry in enumerate(data):
    if i < 2:
        print(entry)

with open(file, 'r', encoding='utf-8') as f:
    raw_text = f.read()

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
docs = text_splitter.create_documents([raw_text])

print(f"Split into {len(docs)} chunks")

!pip install -U langchain-community

!pip install -U langchain-huggingface

!pip install faiss-cpu

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
db = FAISS.from_documents(docs, embeddings)
retriever = db.as_retriever(search_kwargs={"k": 3})

!pip install langchain_openai

from langchain_openai import ChatOpenAI

GROQ_API_KEY = "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"

llm = ChatOpenAI(
    model_name="llama3-70b-8192",
    temperature=0.3,
    max_tokens=512,
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1"
)

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
template = """
You are a helpful assistant who answers questions based ONLY on the provided context.
If the context does NOT contain the answer, respond with:
"I don't have information about this in the provided document."

Context:
{context}

Question:
{question}

Answer:
"""

prompt=PromptTemplate(template=template,input_variables=["context","question"])
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)

def safe_answer(question):
    retrieved_docs = retriever.invoke(question)
    print("Top Retrieved Chunks:")
    for i, doc in enumerate(retrieved_docs):
        print(f"\nChunk {i+1}:\n{doc.page_content[:300]}...")

    if not retrieved_docs or all(len(doc.page_content.strip()) < 10 for doc in retrieved_docs):
        return "I don't have information about this in the provided document."
    else:
        return qa_chain.run(question).strip()

# Test with known topics from the document
questions = [
    "How do I add memory to a RAG application?",
    "What is InceptionV3 used for?",
    "How can I use MongoDB to store chat history?",
    "What is YOLO used for in computer vision?"
]

for q in questions:
    print("\n" + "-"*50)
    print("Question:", q)
    response = safe_answer(q)
    print("Answer:", response)

import gradio as gr

def respond(message, chat_history):
    bot_response = safe_answer(message)
    chat_history.append((message, bot_response))
    return chat_history, chat_history

with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="Your Question")
    clear = gr.Button("Clear Chat")

    state = gr.State([])

    msg.submit(respond, [msg, state], [chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state])

demo.launch()